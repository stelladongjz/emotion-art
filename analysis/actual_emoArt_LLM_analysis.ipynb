{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Emotion + Likeness Rating & Explanation for Paintings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model information: Gemini 2.5 flash\n",
    "### Two prompts are used, a simple prompt and an enhanced prompt with rating score anchoring + explicitly asking the model to pay attention to low level features\n",
    "### output \"emotion_output.csv\" from simple prompt and \"emotion_output_enhanced.csv\" for enhanced prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from google.api_core import exceptions as google_exceptions\n",
    "\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import logging, sys\n",
    "from pathlib import Path\n",
    "import mimetypes\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "%run \"api_key.ipynb\" #import API key as needed\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout,             \n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For each emotion, write one model to produce output. Plus one aesthetic rating model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ——— CONFIGURATION ———————————————————————————————————————————————————————————————————\n",
    "IMAGE_FOLDER      = Path(\"/Users/Stella/Desktop/EmotionArt/emotion-art/data/40_test\")\n",
    "OUTPUT_CSV        = Path(\"emotion_model_output_vertex.csv\")\n",
    "MODEL_NAME        = \"gemini-2.5-flash\"\n",
    "MAX_RETRIES       = 6\n",
    "BACKOFF_BASE      = 5.0\n",
    "BATCH_SIZE        = 10\n",
    "MAX_OUTPUT_TOKENS = 2049\n",
    "IMAGE_EXTS        = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\", \".tiff\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ——— LOGGING ———————————————————————————————————————————————————————————————————————\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout,             \n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ——— HELPER FUNCTIONS —————————————————————————————————————————————————————————————————\n",
    "def get_mime_type(path: str) -> str:\n",
    "    mime, _ = mimetypes.guess_type(path)\n",
    "    return mime or \"application/octet-stream\"\n",
    "\n",
    "def is_image_file(path: Path) -> bool:\n",
    "    return path.is_file() and path.suffix.lower() in IMAGE_EXTS and not path.name.startswith(\".\")\n",
    "\n",
    "_SCORE_RE = re.compile(r\"^[A-Za-z]+:\\s*([\\d.]+)\", re.MULTILINE)\n",
    "_EXPL_RE  = re.compile(r\"Explanation:\\s*(.*)\", re.DOTALL)\n",
    "\n",
    "def parse_response(raw: str) -> tuple[float|None, str]:\n",
    "    text = raw.decode(\"utf-8\", errors=\"ignore\") if isinstance(raw, (bytes, bytearray)) else str(raw or \"\")\n",
    "    m_score = _SCORE_RE.search(text)\n",
    "    m_expl  = _EXPL_RE.search(text)\n",
    "    if not (m_score and m_expl):\n",
    "        return None, text.strip()\n",
    "    return float(m_score.group(1)), m_expl.group(1).strip()\n",
    "\n",
    "#for batch processing\n",
    "def chunker(seq, size):\n",
    "    for i in range(0, len(seq), size):\n",
    "        yield seq[i : i + size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompts\n",
    "def build_emotion_prompt(emotion: str) -> str:\n",
    "    return (\n",
    "        f\"You are an art expert describing your emotional response to a painting.\\n\"\n",
    "        f\"Evaluate **{emotion.lower()}** independently, without reference to any other feeling. \"\n",
    "        \"Do not assume anything about other possible emotional reactions — focus only on this one emotion.\\n\\n\"\n",
    "        \"Provide your response using the following structure:\\n\"\n",
    "        \"1. A **numeric score** between 0 and 100 (on a continuous scale — do not round to nearest 5 or 10 unless warranted)\\n\"\n",
    "        \"2. A **detailed explanation** supporting the reason behind the rating you provided. Please try to be as detailed as possible.\\n\\n\"\n",
    "        f\"Use the format exactly:\\n\"\n",
    "        f\"{emotion}: [score]\\n\"\n",
    "        \"Explanation: ...\"\n",
    "    )\n",
    "\n",
    "# Prompt dictionary for loop call models later on with all emotions and liking rating:\n",
    "prompt_dict = {emotion: build_emotion_prompt(emotion) for emotion in [\"Joy\", \"Sadness\", \"Fear\", \"Anger\", \"Disgust\", \"Surprise\"]}\n",
    "prompt_dict[\"Liking\"] = (\n",
    "    \"You are an art expert evaluating how much you like a painting.\\n\"\n",
    "    \"Rate your **personal aesthetic preference** for the painting, based only on what is visually presented.\\n\"\n",
    "    \"Provide your response using the following structure:\\n\"\n",
    "    \"1. A **numeric score** between 0 and 100 (on a continuous scale — do not round unless appropriate)\\n\"\n",
    "    \"2. A **detailed explanation** supporting the reason behind the rating you provided. Please try to be as detailed as possible.\\n\\n\"\n",
    "    \"Use the format exactly:\\n\"\n",
    "    \"Liking: [score]\\n\"\n",
    "    \"Explanation: ...\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced prompt that defines a clear scale anchors (0, 50, 100)\n",
    "def build_emotion_prompt_enhanced(emotion: str) -> str:\n",
    "    return (\n",
    "        \"You are an expert art critic and psychologist, trained to assess a viewer's emotional response\"\n",
    "        f\"to a painting. Focus **only** on **{emotion.lower()}**—do not blend in any other feeling.\\n\\n\"\n",
    "        \"**Scale definition (0-100):**  \\n\"\n",
    "        f\"- **0** means “no sense of {emotion.lower()} at all.”  \\n\"\n",
    "        \"- **50** means “a moderate, everyday level—what most people might feel in a typical scene.”  \\n\"\n",
    "        f\"- **100** means “an overwhelming, emotionally extreme sense of {emotion.lower()}.”\\n\\n\"\n",
    "        \"**Instructions:**  \\n\"\n",
    "        \"1. Look closely at composition, color palette, lighting, brushwork, subject matter, and style.  \\n\"\n",
    "        \"2. Compare what you see to the anchors above—if it's slightly more than “everyday,” pick something like 60-70; if it barely registers, choose 5-10.  \\n\"\n",
    "        f\"3. Avoid clustering at 50: if the painting truly feels neutral for “{emotion.lower()},” explain why and use exactly 50; otherwise pick a number that reflects the visual evidence.  \\n\"\n",
    "        \"4. If you choose above 85 or below 15, you must justify why it crosses into “extreme” territory.  \\n\"\n",
    "        \"5. **Write exactly five complete sentences** in your explanation—no more, no fewer.  \\n\\n\"\n",
    "        \"Provide your response using the following structure:\\n\"\n",
    "        \"1. A **numeric score** between 0 and 100 (on a continuous scale — do not round unless appropriate)\\n\"\n",
    "        \"2. A **detailed explanation** A detdescription of the visual elements (e.g., “the high-contrast reds and jagged lines give a surge of …”) that led you to that score. \\n\\n\"\n",
    "        \"**Output format (exactly):**  \\n\"\n",
    "        f\"{emotion}: [score]\\n\"\n",
    "        \"Explanation: ...\"\n",
    "    )\n",
    "\n",
    "prompt_dict_enhanced = {\n",
    "    emotion: build_emotion_prompt_enhanced(emotion)\n",
    "    for emotion in [\"Joy\", \"Sadness\", \"Fear\", \"Anger\", \"Disgust\", \"Surprise\"]\n",
    "}\n",
    "\n",
    "prompt_dict_enhanced[\"Liking\"] = (\n",
    "    \"You are an expert art critic rating your own **aesthetic preference** for a painting on a 0-100 scale.\\n\\n\"\n",
    "    \"**Scale definition (0-100):**  \\n\"\n",
    "    \"- **0** means “I wouldn't want this in my home or collection.”  \\n\"\n",
    "    \"- **50** means “it's average—interesting but not memorable.”  \\n\"\n",
    "    \"- **100** means “I find it utterly compelling and would absolutely display it.”\\n\\n\"\n",
    "    \"**Instructions:**  \\n\"\n",
    "    \"1. Consider composition, color harmony, technique, originality, and emotional impact on *you*.  \\n\"\n",
    "    \"2. Anchor your number to the scale above—if your preference is tepid, choose 30-40; if you love it, choose 80-95.  \\n\"\n",
    "    \"3. Avoid mid-range clustering—only use 50 if it truly feels neutral.  \\n\"\n",
    "    \"4. If you go above 90 or below 10, explain why it's so extremely likable or unlikable.  \\n\"\n",
    "    \"5. **Write exactly five complete sentences** in your explanation—no more, no fewer.  \\n\\n\"\n",
    "    \"Provide your response using the following structure:\\n\"\n",
    "    \"1. A **numeric score** between 0 and 100 (on a continuous scale — do not round unless appropriate)\\n\"\n",
    "    \"2. A **detailed explanation** supporting the reason behind the rating you provided. Please try to be as detailed as possible.\\n\\n\"\n",
    "    f\"**Output format (exactly):**  \\n\"\n",
    "    \"Liking: [score]\\n\"\n",
    "    \"Explanation: ...\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/Stella/Desktop/EmotionArt/emotion-art/data/40_test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ——— PRELOAD IMAGES AND PROMPTS ——————————————————————————————————————————————————————————\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m all_images  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m IMAGE_FOLDER\u001b[38;5;241m.\u001b[39miterdir() \u001b[38;5;28;01mif\u001b[39;00m is_image_file(p))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# ——— VERTEX AI CLIENT FACTORY —————————————————————————————————————————————————————————\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_vertex_client\u001b[39m():\n",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ——— PRELOAD IMAGES AND PROMPTS ——————————————————————————————————————————————————————————\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m all_images  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m IMAGE_FOLDER\u001b[38;5;241m.\u001b[39miterdir() \u001b[38;5;28;01mif\u001b[39;00m is_image_file(p))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# ——— VERTEX AI CLIENT FACTORY —————————————————————————————————————————————————————————\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_vertex_client\u001b[39m():\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/pathlib.py:1056\u001b[0m, in \u001b[0;36mPath.iterdir\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21miterdir\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Yield path objects of the directory contents.\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m \n\u001b[1;32m   1053\u001b[0m \u001b[38;5;124;03m    The children are yielded in arbitrary order, and the\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;124;03m    special entries '.' and '..' are not included.\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1056\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1057\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_child_relpath(name)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/Stella/Desktop/EmotionArt/emotion-art/data/40_test'"
     ]
    }
   ],
   "source": [
    "# ——— PRELOAD IMAGES AND PROMPTS ——————————————————————————————————————————————————————————\n",
    "all_images  = sorted(p for p in IMAGE_FOLDER.iterdir() if is_image_file(p))\n",
    "\n",
    "# ——— VERTEX AI CLIENT FACTORY —————————————————————————————————————————————————————————\n",
    "def make_vertex_client():\n",
    "    return genai.Client(\n",
    "        vertexai=True,\n",
    "        project=\"emotion-art-analysis\",\n",
    "        location=\"global\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ——— CORE EMOTION CALL —————————————————————————————————————————————————————————————————\n",
    "def emotion_model_vertex(image_path: str, prompt_text: str, emotion_label: str, client) -> dict:\n",
    "    with open(image_path,\"rb\") as img_file:\n",
    "        image_bytes = img_file.read()\n",
    "        \n",
    "    mime_type = get_mime_type(image_path)\n",
    "\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            resp = client.models.generate_content(\n",
    "                model   = MODEL_NAME,\n",
    "                contents = [\n",
    "                    types.Part.from_bytes(data=image_bytes, mime_type=mime_type),\n",
    "                    types.Part.from_text(text=prompt_text),\n",
    "                ],\n",
    "                config = types.GenerateContentConfig(\n",
    "                    max_output_tokens=MAX_OUTPUT_TOKENS\n",
    "                )\n",
    "            )\n",
    "\n",
    "            raw = resp.candidates[0].content.parts[0].text if resp.candidates else \"\"\n",
    "            score, explanation = parse_response(raw)\n",
    "            return {\n",
    "                \"image\": Path(image_path).name,\n",
    "                f\"{emotion_label.lower()}_rating\":      score,\n",
    "                f\"{emotion_label.lower()}_explanation\": explanation\n",
    "            }\n",
    "\n",
    "        except google_exceptions.ServiceUnavailable:\n",
    "            backoff = min(BACKOFF_BASE * 2 ** (attempt - 1), 30) + random.random()\n",
    "            logging.warning(f\"503 overload (try {attempt}), sleeping {backoff:.1f}s\")\n",
    "            time.sleep(backoff)\n",
    "\n",
    "        except Exception as e:\n",
    "            backoff = min(BACKOFF_BASE * 2 ** (attempt - 1), 30) + random.random()\n",
    "            logging.warning(f\"Error on try {attempt}: {e}\")\n",
    "            time.sleep(backoff)\n",
    "\n",
    "    return {\n",
    "        \"image\": image_path.name,\n",
    "        f\"{emotion_label.lower()}_rating\":      None,\n",
    "        f\"{emotion_label.lower()}_explanation\": f\"ERROR: model overloaded after {MAX_RETRIES} tries\"\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ——— BATCHED WRAPPER —————————————————————————————————————————————————————————————————\n",
    "def get_response_by_emotion_vertex(emotion_label: str, prompt_dict, batch_size: int = BATCH_SIZE):\n",
    "    client = make_vertex_client()\n",
    "    records = []\n",
    "    total   = len(all_images)\n",
    "    total_batches = (total+batch_size-1) // batch_size\n",
    "\n",
    "    for b_idx, batch in enumerate(chunker(all_images, batch_size), start=1):\n",
    "        logging.info(f\"Starting {emotion_label} batch {b_idx}/{total_batches} (size={len(batch)})\")\n",
    "        for i, img_path in enumerate(batch, start=1):\n",
    "            idx = (b_idx - 1) * batch_size + i\n",
    "            name = img_path.name\n",
    "            logging.info(f\"[{idx}/{total}] {emotion_label}: Processing {name}\")\n",
    "\n",
    "            try:\n",
    "                rec = emotion_model_vertex(str(img_path), prompt_dict[emotion_label], emotion_label,client)\n",
    "                logging.info(f\"→ {emotion_label} Success: {name} → rating={rec[f'{emotion_label.lower()}_rating']}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"❌ {emotion_label} error on {name}: {e}\", exc_info=True)\n",
    "                rec = {\n",
    "                    \"image\":               name,\n",
    "                    f\"{emotion_label.lower()}_rating\":      \"\",\n",
    "                    f\"{emotion_label.lower()}_explanation\": f\"ERROR: {e}\"\n",
    "                }\n",
    "\n",
    "            records.append(rec)\n",
    "    df = pd.DataFrame(records)\n",
    "    #df.to_csv(OUTPUT_CSV.with_stem(f\"{emotion_label.lower()}_vertex\"), index=False)\n",
    "    print(f\"✅ {emotion_label} (Vertex) results saved to {df.shape[0]} rows\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process 50 images test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Joy (Vertex) results saved to 50 rows\n"
     ]
    }
   ],
   "source": [
    "joy_50 = get_response_by_emotion_vertex(\"Joy\", prompt_dict)\n",
    "joy_50.to_csv(\"../output/joy_50.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sadness (Vertex) results saved to 50 rows\n"
     ]
    }
   ],
   "source": [
    "sadness_50 = get_response_by_emotion_vertex(\"Sadness\", prompt_dict)\n",
    "sadness_50.to_csv(\"../output/sadness_50.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23:16:40 [WARNING] Error on try 1: 'NoneType' object is not subscriptable\n",
      "✅ Fear (Vertex) results saved to 50 rows\n"
     ]
    }
   ],
   "source": [
    "fear_50 = get_response_by_emotion_vertex(\"Fear\", prompt_dict)\n",
    "fear_50.to_csv(\"../output/fear_50.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23:30:16 [WARNING] Error on try 1: 'NoneType' object is not subscriptable\n",
      "23:30:42 [WARNING] Error on try 2: 'NoneType' object is not subscriptable\n",
      "✅ Anger (Vertex) results saved to 50 rows\n"
     ]
    }
   ],
   "source": [
    "anger_50 = get_response_by_emotion_vertex(\"Anger\", prompt_dict)\n",
    "anger_50.to_csv(\"../output/anger_50.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Disgust (Vertex) results saved to 50 rows\n"
     ]
    }
   ],
   "source": [
    "disgust_50 = get_response_by_emotion_vertex(\"Disgust\", prompt_dict)\n",
    "disgust_50.to_csv(\"../output/disgust_50.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23:45:17 [WARNING] Error on try 1: 'NoneType' object is not subscriptable\n",
      "✅ Surprise (Vertex) results saved to 50 rows\n"
     ]
    }
   ],
   "source": [
    "surprise_50 = get_response_by_emotion_vertex(\"Surprise\", prompt_dict)\n",
    "surprise_50.to_csv(\"../output/surprise_50.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Liking (Vertex) results saved to 50 rows\n"
     ]
    }
   ],
   "source": [
    "liking_50 = get_response_by_emotion_vertex(\"Liking\", prompt_dict)\n",
    "liking_50.to_csv(\"../output/liking_50.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------END-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 882 images in total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
