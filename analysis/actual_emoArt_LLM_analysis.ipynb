{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Emotion + Likeness Rating & Explanation for Paintings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model information: Gemini 2.5 flash\n",
    "### Two prompts are used, a simple prompt and an enhanced prompt with rating score anchoring + explicitly asking the model to pay attention to low level features\n",
    "### output \"emotion_output.csv\" from simple prompt and \"emotion_output_enhanced.csv\" for enhanced prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from google.api_core import exceptions as google_exceptions\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import logging, sys\n",
    "from pathlib import Path\n",
    "import mimetypes\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from functools import reduce\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout,             \n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For each emotion, write one model to produce output. Plus one aesthetic rating model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ——— CONFIGURATION ———————————————————————————————————————————————————————————————————\n",
    "IMAGE_FOLDER      = Path(\"/Users/Stella/Desktop/EmotionArt/emotion-art/data/exp_images/Cubism\")\n",
    "OUTPUT_CSV        = Path(\"emotion_model_output.csv\")\n",
    "MODEL_NAME        = \"gemini-2.5-flash\"\n",
    "MAX_RETRIES       = 6\n",
    "BACKOFF_BASE      = 3.0\n",
    "BATCH_SIZE        = 10\n",
    "MAX_OUTPUT_TOKENS = 2049\n",
    "IMAGE_EXTS        = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\", \".tiff\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ——— LOGGING ———————————————————————————————————————————————————————————————————————\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout,             \n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ——— HELPER FUNCTIONS —————————————————————————————————————————————————————————————————\n",
    "def get_mime_type(path: str) -> str:\n",
    "    mime, _ = mimetypes.guess_type(path)\n",
    "    return mime or \"application/octet-stream\"\n",
    "\n",
    "def is_image_file(path: Path) -> bool:\n",
    "    return path.is_file() and path.suffix.lower() in IMAGE_EXTS and not path.name.startswith(\".\")\n",
    "\n",
    "_SCORE_RE = re.compile(r\"^[A-Za-z]+:\\s*([\\d.]+)\", re.MULTILINE)\n",
    "_EXPL_RE  = re.compile(r\"Explanation:\\s*(.*)\", re.DOTALL)\n",
    "\n",
    "def parse_response(raw: str) -> tuple[float|None, str]:\n",
    "    text = raw.decode(\"utf-8\", errors=\"ignore\") if isinstance(raw, (bytes, bytearray)) else str(raw or \"\")\n",
    "    m_score = _SCORE_RE.search(text)\n",
    "    m_expl  = _EXPL_RE.search(text)\n",
    "    if not (m_score and m_expl):\n",
    "        return None, text.strip()\n",
    "    return float(m_score.group(1)), m_expl.group(1).strip()\n",
    "\n",
    "#for batch processing\n",
    "def chunker(seq, size):\n",
    "    for i in range(0, len(seq), size):\n",
    "        yield seq[i : i + size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompts\n",
    "def build_emotion_prompt(emotion: str) -> str:\n",
    "    return (\n",
    "        f\"You are an art expert describing your emotional response to a painting.\\n\"\n",
    "        f\"Evaluate **{emotion.lower()}** independently, without reference to any other feeling. \"\n",
    "        \"Do not assume anything about other possible emotional reactions — focus only on this one emotion.\\n\\n\"\n",
    "        \"Provide your response using the following structure:\\n\"\n",
    "        \"1. A **numeric score** between 0 and 100 (on a continuous scale — do not round to nearest 5 or 10 unless warranted)\\n\"\n",
    "        \"2. A **detailed explanation** supporting the reason behind the rating you provided. Please try to be as detailed as possible.\\n\\n\"\n",
    "        f\"Use the format exactly:\\n\"\n",
    "        f\"{emotion}: [score]\\n\"\n",
    "        \"Explanation: ...\"\n",
    "    )\n",
    "\n",
    "# Prompt dictionary for loop call models later on with all emotions and liking rating:\n",
    "prompt_dict = {emotion: build_emotion_prompt(emotion) for emotion in [\"Joy\", \"Sadness\", \"Fear\", \"Anger\", \"Disgust\", \"Surprise\"]}\n",
    "prompt_dict[\"Liking\"] = (\n",
    "    \"You are an art expert evaluating how much you like a painting.\\n\"\n",
    "    \"Rate your **personal aesthetic preference** for the painting, based only on what is visually presented.\\n\"\n",
    "    \"Provide your response using the following structure:\\n\"\n",
    "    \"1. A **numeric score** between 0 and 100 (on a continuous scale — do not round unless appropriate)\\n\"\n",
    "    \"2. A **detailed explanation** supporting the reason behind the rating you provided. Please try to be as detailed as possible.\\n\\n\"\n",
    "    \"Use the format exactly:\\n\"\n",
    "    \"Liking: [score]\\n\"\n",
    "    \"Explanation: ...\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced prompt that defines a clear scale anchors (0, 50, 100)\n",
    "def build_emotion_prompt_enhanced(emotion: str) -> str:\n",
    "    return (\n",
    "        \"You are an expert art critic and psychologist, trained to assess a viewer's emotional response\"\n",
    "        f\"to a painting. Focus **only** on **{emotion.lower()}**—do not blend in any other feeling.\\n\\n\"\n",
    "        \"**Scale definition (0-100):**  \\n\"\n",
    "        f\"- **0** means “no sense of {emotion.lower()} at all.”  \\n\"\n",
    "        \"- **50** means “a moderate, everyday level—what most people might feel in a typical scene.”  \\n\"\n",
    "        f\"- **100** means “an overwhelming, emotionally extreme sense of {emotion.lower()}.”\\n\\n\"\n",
    "        \"**Instructions:**  \\n\"\n",
    "        \"1. Look closely at composition, color palette, lighting, brushwork, subject matter, and style.  \\n\"\n",
    "        \"2. Compare what you see to the anchors above—if it's slightly more than “everyday,” pick something like 60-70; if it barely registers, choose 5-10.  \\n\"\n",
    "        f\"3. Avoid clustering at 50: if the painting truly feels neutral for “{emotion.lower()},” explain why and use exactly 50; otherwise pick a number that reflects the visual evidence.  \\n\"\n",
    "        \"4. If you choose above 85 or below 15, you must justify why it crosses into “extreme” territory.  \\n\"\n",
    "        \"5. **Write exactly five complete sentences** in your explanation—no more, no fewer.  \\n\\n\"\n",
    "        \"Provide your response using the following structure:\\n\"\n",
    "        \"1. A **numeric score** between 0 and 100 (on a continuous scale — do not round unless appropriate)\\n\"\n",
    "        \"2. A **detailed explanation** A detdescription of the visual elements (e.g., “the high-contrast reds and jagged lines give a surge of …”) that led you to that score. \\n\\n\"\n",
    "        \"**Output format (exactly):**  \\n\"\n",
    "        f\"{emotion}: [score]\\n\"\n",
    "        \"Explanation: ...\"\n",
    "    )\n",
    "\n",
    "prompt_dict_enhanced = {\n",
    "    emotion: build_emotion_prompt_enhanced(emotion)\n",
    "    for emotion in [\"Joy\", \"Sadness\", \"Fear\", \"Anger\", \"Disgust\", \"Surprise\"]\n",
    "}\n",
    "\n",
    "prompt_dict_enhanced[\"Liking\"] = (\n",
    "    \"You are an expert art critic rating your own **aesthetic preference** for a painting on a 0-100 scale.\\n\\n\"\n",
    "    \"**Scale definition (0-100):**  \\n\"\n",
    "    \"- **0** means “I wouldn't want this in my home or collection.”  \\n\"\n",
    "    \"- **50** means “it's average—interesting but not memorable.”  \\n\"\n",
    "    \"- **100** means “I find it utterly compelling and would absolutely display it.”\\n\\n\"\n",
    "    \"**Instructions:**  \\n\"\n",
    "    \"1. Consider composition, color harmony, technique, originality, and emotional impact on *you*.  \\n\"\n",
    "    \"2. Anchor your number to the scale above—if your preference is tepid, choose 30-40; if you love it, choose 80-95.  \\n\"\n",
    "    \"3. Avoid mid-range clustering—only use 50 if it truly feels neutral.  \\n\"\n",
    "    \"4. If you go above 90 or below 10, explain why it's so extremely likable or unlikable.  \\n\"\n",
    "    \"5. **Write exactly five complete sentences** in your explanation—no more, no fewer.  \\n\\n\"\n",
    "    \"Provide your response using the following structure:\\n\"\n",
    "    \"1. A **numeric score** between 0 and 100 (on a continuous scale — do not round unless appropriate)\\n\"\n",
    "    \"2. A **detailed explanation** supporting the reason behind the rating you provided. Please try to be as detailed as possible.\\n\\n\"\n",
    "    f\"**Output format (exactly):**  \\n\"\n",
    "    \"Liking: [score]\\n\"\n",
    "    \"Explanation: ...\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ——— PRELOAD IMAGES AND PROMPTS ——————————————————————————————————————————————————————————\n",
    "all_images  = sorted(p for p in IMAGE_FOLDER.iterdir() if is_image_file(p))\n",
    "\n",
    "# ——— VERTEX AI CLIENT FACTORY —————————————————————————————————————————————————————————\n",
    "def make_vertex_client():\n",
    "    return genai.Client(\n",
    "        vertexai=True,\n",
    "        project=\"emotion-art-analysis\",\n",
    "        location=\"global\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of .iterdir(), use .rglob() to find images in any subdirectory\n",
    "all_images = sorted(\n",
    "    p for p in IMAGE_FOLDER.rglob(\"*\")\n",
    "    if is_image_file(p)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ——— CORE EMOTION CALL —————————————————————————————————————————————————————————————————\n",
    "def emotion_model_vertex(image_path: str, prompt_text: str, emotion_label: str, client) -> dict:\n",
    "    \n",
    "    image_path = Path(image_path)\n",
    "    image_name = image_path.name\n",
    "    image_category = image_path.parent.name\n",
    "    with open(image_path,\"rb\") as img_file:\n",
    "        image_bytes = img_file.read()\n",
    "        \n",
    "    mime_type = get_mime_type(image_path)\n",
    "\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            resp = client.models.generate_content(\n",
    "                model   = MODEL_NAME,\n",
    "                contents = [\n",
    "                    types.Part.from_bytes(data=image_bytes, mime_type=mime_type),\n",
    "                    types.Part.from_text(text=prompt_text),\n",
    "                ],\n",
    "                config = types.GenerateContentConfig(\n",
    "                    max_output_tokens=MAX_OUTPUT_TOKENS\n",
    "                )\n",
    "            )\n",
    "\n",
    "            raw = resp.candidates[0].content.parts[0].text if resp.candidates else \"\"\n",
    "            score, explanation = parse_response(raw)\n",
    "            return {\n",
    "                \"image\": image_name,\n",
    "                \"image_category\": image_category,\n",
    "                f\"{emotion_label.lower()}_rating\":      score,\n",
    "                f\"{emotion_label.lower()}_explanation\": explanation\n",
    "            }\n",
    "\n",
    "        except google_exceptions.ServiceUnavailable:\n",
    "            backoff = min(BACKOFF_BASE * 2 ** (attempt - 1), 30) + random.random()\n",
    "            logging.warning(f\"503 overload (try {attempt}), sleeping {backoff:.1f}s\")\n",
    "            time.sleep(backoff)\n",
    "\n",
    "        except Exception as e:\n",
    "            backoff = min(BACKOFF_BASE * 2 ** (attempt - 1), 30) + random.random()\n",
    "            logging.warning(f\"Error on try {attempt}: {e}\")\n",
    "            time.sleep(backoff)\n",
    "\n",
    "    return {\n",
    "        \"image\": image_name,\n",
    "        \"image_category\": image_category,\n",
    "        f\"{emotion_label.lower()}_rating\":      None,\n",
    "        f\"{emotion_label.lower()}_explanation\": f\"ERROR: model overloaded after {MAX_RETRIES} tries\"\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ——— BATCHED WRAPPER —————————————————————————————————————————————————————————————————\n",
    "def get_response_by_emotion_vertex(emotion_label: str, prompt_dict, batch_size: int = BATCH_SIZE):\n",
    "    client = make_vertex_client()\n",
    "    records = []\n",
    "    total   = len(all_images)\n",
    "    total_batches = (total+batch_size-1) // batch_size\n",
    "\n",
    "    for b_idx, batch in enumerate(chunker(all_images, batch_size), start=1):\n",
    "        logging.info(f\"Starting {emotion_label} batch {b_idx}/{total_batches} (size={len(batch)})\")\n",
    "        for i, img_path in enumerate(batch, start=1):\n",
    "            idx = (b_idx - 1) * batch_size + i\n",
    "            name = img_path.name\n",
    "            logging.info(f\"[{idx}/{total}] {emotion_label}: Processing {name}\")\n",
    "\n",
    "            try:\n",
    "                rec = emotion_model_vertex(str(img_path), prompt_dict[emotion_label], emotion_label,client)\n",
    "                logging.info(f\"→ {emotion_label} Success: {name} → rating={rec[f'{emotion_label.lower()}_rating']}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"❌ {emotion_label} error on {name}: {e}\", exc_info=True)\n",
    "                rec = {\n",
    "                    \"image\":               name,\n",
    "                    f\"{emotion_label.lower()}_rating\":      \"\",\n",
    "                    f\"{emotion_label.lower()}_explanation\": f\"ERROR: {e}\"\n",
    "                }\n",
    "\n",
    "            records.append(rec)\n",
    "    df = pd.DataFrame(records)\n",
    "    #df.to_csv(OUTPUT_CSV.with_stem(f\"{emotion_label.lower()}_vertex\"), index=False)\n",
    "    print(f\"✅ {emotion_label} (Vertex) results saved to {df.shape[0]} rows\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process images & save output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Processing Joy in parallel (max 8 workers) ===\n",
      "17:36:23 [WARNING] Error on try 1: 'NoneType' object is not subscriptable\n",
      "17:37:12 [WARNING] Error on try 1: 'NoneType' object is not subscriptable\n",
      "17:37:34 [WARNING] Error on try 2: 'NoneType' object is not subscriptable\n",
      "17:39:18 [WARNING] Error on try 1: 'NoneType' object is not subscriptable\n",
      "[Joy] saved 201 rows to joy_Cubism.csv\n",
      "=== Processing Sadness in parallel (max 8 workers) ===\n"
     ]
    }
   ],
   "source": [
    "#paralell processing w 8 workders \n",
    "\n",
    "output_dir = Path(\"../output\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MAX_WORKERS = 8  # tune this\n",
    "\n",
    "for emotion in prompt_dict.keys():\n",
    "    print(f\"=== Processing {emotion} in parallel (max {MAX_WORKERS} workers) ===\")\n",
    "    client = make_vertex_client()\n",
    "\n",
    "    # submit one future per image\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        future_to_img = {\n",
    "            executor.submit(\n",
    "                emotion_model_vertex,\n",
    "                img_path, \n",
    "                prompt_dict[emotion], \n",
    "                emotion, \n",
    "                client\n",
    "            ): img_path\n",
    "            for img_path in all_images\n",
    "        }\n",
    "\n",
    "        records = []\n",
    "        for future in as_completed(future_to_img):\n",
    "            img_path = future_to_img[future]\n",
    "            try:\n",
    "                rec = future.result()\n",
    "                logging.info(f\"→ {emotion} {img_path.name} → rating={rec[f'{emotion.lower()}_rating']}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"❌ {emotion} error on {img_path.name}: {e}\", exc_info=True)\n",
    "                rec = {\n",
    "                    \"image\":            img_path.name,\n",
    "                    \"image_category\":   img_path.parent.name,\n",
    "                    f\"{emotion.lower()}_rating\":      None,\n",
    "                    f\"{emotion.lower()}_explanation\": str(e)\n",
    "                }\n",
    "            records.append(rec)\n",
    "\n",
    "    # build your DataFrame and write one CSV per category\n",
    "    df = pd.DataFrame(records)\n",
    "    for category, df_cat in df.groupby(\"image_category\"):\n",
    "        fname = f\"{emotion.lower()}_{category}.csv\"\n",
    "        df_cat.to_csv(output_dir / fname, index=False)\n",
    "        print(f\"[{emotion}] saved {len(df_cat)} rows to {fname}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------END-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 882 images in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
